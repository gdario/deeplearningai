{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd7c4c87",
   "metadata": {},
   "source": [
    "# Lesson 1: Router Engine\n",
    "\n",
    "We will start with the simplest form of agentic RAG, a router. Given a query, a router will pick one of several query engines that execute the query. We will build a simple router over a single document that can handle question answering and summarization.\n",
    "\n",
    "![Router Engine](img/router_engine.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877df988",
   "metadata": {},
   "source": [
    "Welcome to Lesson 1.\n",
    "\n",
    "The first three lesson will build agentic capabilities over a single document. The fourth lesson will show how to build a multi-document agent.\n",
    "To access the `requirements.txt` file, the data/pdf file required for this lesson and the `helper` and `utils` modules, please go to the `File` menu and select`Open...`.\n",
    "\n",
    "I hope you enjoy this course!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357c97c9",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We must import `nest_asyncio` because Jupyter runs a loop behind the scenes, and this loop leverages async. To make async play nice with Jupyter notebooks, we need this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab5f4f4a-5890-451c-8869-24606ef9f396",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from helper import get_openai_api_key\n",
    "\n",
    "OPENAI_API_KEY = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffc9b4f4-64d4-4266-9889-54db90e00ee9",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fca250",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ae2a8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "This is the paper we will use in the first three lessons.\n",
    "To download this paper, below is the needed code:\n",
    "\n",
    "#!wget \"https://openreview.net/pdf?id=VtmBAGCN7o\" -O metagpt.pdf\n",
    "\n",
    "**Note**: The pdf file is included with this lesson. To access it, go to the `File` menu and select`Open...`.\n",
    "\n",
    "The `SimpleDirectoryReader` function reads a PDF file into a parsed document representation. You can find more about this function [here](https://docs.llamaindex.ai/en/stable/module_guides/loading/simpledirectoryreader/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c7f012d-dcd3-4881-a568-72dd27d79159",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(input_files=[\"metagpt.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b48a301",
   "metadata": {},
   "source": [
    "## Define LLM and Embedding model\n",
    "\n",
    "We split the document trying to respect the boundaries of sentences, but with a chunks size limit of 1024 (tokens). More information on `SentenceSplitter` can be found [here](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/?h=sentencesplitter#sentencesplitter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a537bc0-78ee-4dda-a43f-60fd80062df6",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962c258c",
   "metadata": {},
   "source": [
    "LamaIndex, by default, uses gpt-3.5-turbo as a model and the `text-embedding-ada-002` as the embedding model, but this can be fully customized. Customization is done using the `Settings` module from `llama_index.core`. and setting the fields `.llm` and `.embed_model`. The code below shows how to change the LLM and the embeddings.\n",
    "\n",
    "**Question**: is it only possible to consider the LLMs and embedding models stored in the `llms` and `embeddings` modules of `llama_index`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de0660ee-b231-4351-b158-d8ad023e00b5",
   "metadata": {
    "height": 115,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997c7559",
   "metadata": {},
   "source": [
    "## Define Summary Index and Vector Index over the Same Data\n",
    "\n",
    "Here we build two indexes, one for Q&A and one for summarization. An index can be thought of as metadata over our data. Indexes can be queried, and different indexes will have different retrieval behaviors.\n",
    "\n",
    "A vector index is based on embeddings, and retrieval is based on embedding similarity. It is a core abstraction for any RAG system.\n",
    "\n",
    "A summary index will return all the nodes currently in the index, so it doesn't really depend on the user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73d01b01-bc74-432a-8d92-07b9e86498b0",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9898d3f",
   "metadata": {},
   "source": [
    "## Define Query Engines and Set Metadata\n",
    "\n",
    "The next step is turning these indexes into query engines and query tools. Each query engine is good for a certain type of query, and routers allow to route different queries to different engines. Below we define the summary query engine.\n",
    "\n",
    "A query tool is just a query engine with metadata. It is a description of what types of questions the tool can answer. We will define a tool for the summary and Q&A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44cd7046-c714-4920-b077-b3ded917862f",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "vector_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a1d6d75-247e-426a-8ef4-b49225c24796",
   "metadata": {
    "height": 285,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful for summarization questions related to MetaGPT\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context from the MetaGPT paper.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d2c152",
   "metadata": {},
   "source": [
    "## Define Router Query Engine\n",
    "\n",
    "LlamaIndex provides several types of *selectors* to build a router, each of which has distinct attributes.\n",
    "\n",
    "The LLM selectors use the LLM to output a JSON that is parsed, and the corresponding indexes are queried.\n",
    "\n",
    "Instead of directly prompting the LLM with text, the Pydantic selectors use the OpenAI Function Calling API to produce pydantic selection objects, rather than parsing raw JSON. Let's try an LLM-powered single selector called `LLMSingleSelector`.\n",
    "\n",
    "The `RouterQueryEngine` takes a selector and a list of query engine tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00734d7c-638a-4d63-ab1f-7f5a92a65119",
   "metadata": {
    "height": 217,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        summary_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e27f2e",
   "metadata": {},
   "source": [
    "Let's test some queries. The first query asks for a summary of the document. The router correctly selects the summarization tool. The verbose output allows to view the intermediate steps taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe3f0a76-68a8-444d-867f-d084bb3ff112",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: Useful for summarization questions related to MetaGPT.\n",
      "\u001b[0mThe document introduces MetaGPT, a meta-programming framework that enhances multi-agent systems using Large Language Models (LLMs) by incorporating human-like Standardized Operating Procedures (SOPs) to streamline workflows and improve problem-solving processes. MetaGPT assigns specific roles to agents, facilitates structured communication, and employs an executable feedback mechanism to iteratively improve code quality. It achieves state-of-the-art performance in code generation benchmarks, emphasizing role specialization, workflow management, and efficient sharing mechanisms. The framework also explores self-improvement mechanisms, recursive self-improvement in software development teams, and the concept of multi-agent economies. Additionally, it discusses the software development process for creating a \"Drawing App\" using MetaGPT, highlighting the roles of different agents and the implementation approach. The document evaluates the performance of GPT models, addressing their sensitivity to prompts, code parsers, and ethical concerns. Overall, the document showcases MetaGPT's capabilities in enhancing multi-agent collaborations, software development, and performance evaluation in various benchmarks.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is the summary of the document?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df8b295",
   "metadata": {},
   "source": [
    "The response come with sources that we can inspect with `response.source_nodes`. The number of nodes, here, is the number of chunks the document was split into. This is what we meant when we said that the summary engine returns all its nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3fedea0-f2a9-46bb-8aaf-287df65b8fff",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "print(len(response.source_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d27fe84",
   "metadata": {},
   "source": [
    "The query below, instead, activates the vector tool. Again, the verbose output shows the steps followed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af8c31b3-8e22-4ad9-9825-b8de21bd03c0",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: This choice is more relevant as it specifically mentions retrieving specific context, which is necessary for understanding how agents share information with other agents..\n",
      "\u001b[0mAgents share information with other agents by utilizing a shared message pool where they can publish structured messages. This shared message pool allows all agents to exchange messages directly, enabling them to not only publish their own messages but also access messages from other entities transparently. Additionally, agents can subscribe to relevant messages based on their role profiles, allowing them to extract the information they need based on their specific roles and interests.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"How do agents share information with other agents?\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed060ee",
   "metadata": {},
   "source": [
    "## Let's put everything together\n",
    "\n",
    "The above steps can be summarized in a single helper function that is already available in the `utils` module: the `get_router_query_engine()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8f92e0b-1c54-489b-b8dd-41ebaafb380a",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from utils import get_router_query_engine\n",
    "\n",
    "query_engine = get_router_query_engine(\"metagpt.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec1a43f3-77dc-472a-8adc-56551c00a0ff",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: Ablation study results are specific context from the MetaGPT paper, making choice 2 the most relevant..\n",
      "\u001b[0mThe ablation study results show that MetaGPT effectively addresses challenges related to context utilization, code hallucinations, and information overload in software development. By focusing on unfolding natural language descriptions accurately, maintaining information validity, and using a global message pool with a subscription mechanism, MetaGPT enhances the efficiency and relevance of communication and task-solving processes.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Tell me about the ablation study results?\")\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
